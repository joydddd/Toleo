# Configuration file for Xeon Gold 6326 Processor
# https://ark.intel.com/content/www/us/en/ark/products/212289/intel-xeon-platinum-8368q-processor-57m-cache-2-60-ghz.html
# 64GB EPC 
# https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%206328H.html


#include nehalem

[perf_model/core]
frequency = 3.50

[perf_model/l1_dcache]
cache_size = 32 # in KB
data_access_time = 4
tags_access_time = 1 # total latency: 5 cycles. 

[perf_model/l1_icache]
cache_size = 32 # in KB
data_access_time = 4
tags_access_time = 1 # total latency: 5 cycles. 

[perf_model/l2_cache]
cache_size = 1024 # in KB = 1.0 MB
data_access_time = 10
tags_access_time = 4 # total latency = 14

[perf_model/l3_cache]
perfect = false
cache_block_size = 64
dvfs_domain = global
cache_size = 22528 # in KB = 22MB shared
associativity = 16
address_hash = mask
replacement_policy = lru
# total l3 access time 20.8 ns = 73 cycles @3.5Hz according to membench, +L1+L2 tag times (5 cycles)
data_access_time = 51
tags_access_time = 17
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
num_controllers = 3
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time (22 cycles = 6.3 ns), directory access time,
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Intel says 85 ns (local) and 139 ns (remote) (this is a NUMA system)
# Membench says 367.8 cycles @ 3.5 GHz = 105 ns / 100.6 ns
latency = 94.3
per_controller_bandwidth = 38.9             # In GB/s, as measured by anandtech
chips_per_dimm = 8
dimms_per_controller = 4

mme_enable = true

[perf_model/mme]
mult_latency = 2.3
aes_latency = 11.4

[perf_model/vn_server]
mult_latency = 2.3
aes_latency = 11.4

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus] # for multi-core communication, not important
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

