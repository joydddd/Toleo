# Configuration file for AMD EPYC 9754, Zen4c (small core) architecture
# https://www.amd.com/en/products/cpu/amd-epyc-9754
# Total core: 128
# 1/4 scale down of EPYC 9754

#include nehalem
[clock_skew_minimization]
scheme = barrier
report = true


[clock_skew_minimization/barrier]
quantum = 100

[general]
total_cores = 32

[perf_model/core]
frequency = 2.25 # in GHz

# Zen 4c shares has same IPC as Zen 4, except lower clock frequency
# https://www.semianalysis.com/p/zen-4c-amds-response-to-hyperscale

# As modeled of chips and cheese blog 
# https://chipsandcheese.com/2022/11/05/amds-zen-4-part-1-frontend-and-execution-engine/

[perf_model/core/interval_timer]
dispatch_width = 6 # 6-way super scalar
window_size = 320 # ROB size
num_outstanding_loadstores = 136 # 136 entry load validation queue, 88 entry load execution queue, 64 entry store queue


# This section describes the number of cycles for
# various arithmetic instructions.
# Modled as Zen 3, from chip and cheese blog 
# https://chipsandcheese.com/2021/12/21/gracemont-revenge-of-the-atom-cores/
[perf_model/core/static_instruction_costs]
fadd=3
fsub=3
fmul=3
fdiv=5


# chip and cheese blog 
# https://chipsandcheese.com/2022/11/08/amds-zen-4-part-2-memory-subsystem-and-conclusion/

[perf_model/cache]
levels = 3


# L1D & L1I: 32KB/core x 128 cores, 4 cycle lat., 8-way set associative
[perf_model/l1_dcache]
cache_size = 32 # in KB
data_access_time = 3
tags_access_time = 1
associativity = 8
address_hash = mod
next_level_read_bandwidth = 648 # in bits/cycle L2 bandwidth = 182.28 GB/s

[perf_model/l1_icache]
cache_size = 32 # in KB
data_access_time = 3
tags_access_time = 1
associativity = 8


# L2: 1MB/core (Zen 4c, 2MB/core for Zen 4) x 128 cores, 14 cycle lat., 8-way set associative 
[perf_model/l2_cache]
cache_size = 1024 # in KB = 1.0 MB
data_access_time = 10
tags_access_time = 4 # total latency = 14
address_hash = mod
associative = 8
next_level_read_bandwidth = 547 # in bits/cycle L3 bandwidth = 154.43 GB/s


# L3: cache  16 ccx x 16MB/ccx
# Latency 2MB page 49.32 cycles, 
[perf_model/l3_cache]
perfect = false
cache_block_size = 64 # in bytes
cache_size = 16384          # In KB (16MB)
associativity = 16
address_hash = mask
replacement_policy = lru
tags_access_time = 12       # In cycles
data_access_time = 37       # In cycles, parallel with tag access
perf_model_type = parallel
writethrough = 0
shared_cores = 8            # Number of cores sharing this cache

[perf_model/dram_directory]
locations = llc # place dram controller at llc
# total_entries = number of entries per directory controller 
total_entries = 1048576 # 1M entries, total l3 cache size = 4 x 16MB / 64B cacheline = 1M
associativity = 16
directory_type = full_map

# ## DDR5-4800, 12 ch total, 3 per MC, 4 MCs.
# # DDR5-4800 = 38.4 GB/s / ch, 76.8 GB/s / MC
# # 128GB/dim, 256GB/ch, 3 ch/MC x 4 MCs
# # 1/4 scale down to 3 chs, 1 MC
# [perf_model/dram]
# type = dramsim
# num_controllers = 1 # 1 memory controllers
# # DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time,
# # or network time. Pond Paper says 40ns + 45 ns
# # Intel says 85 ns (local) and 139 ns (remote) (this is a NUMA system)
# # Membench says 197 cycles @ 2.3 GHz = 85.6 ns total
# default_latency = 45
# per_controller_size = 768                 # in GB (128GB/dimm, 256GB/ch, 768GB/MC)
# chips_per_dimm = 8
# dimms_per_controller = 6

# [perf_model/dram/dramsim]
# # TODO: use DDR5_4800 config
# config=/home/joydong/Desktop/spec/vnsniper/DRAMsim3/configs/DDR4_8Gb_x16_3200.ini
# channles_per_contoller= 3
# epoch=10000
# output_dir=.
# frequency=1600 # in MT/s


## DEBUG: const dram model
[perf_model/dram]
per_controller_size = 768 # in GB
type = constant
num_controllers = 1                     
latency = 45                                # In nanoseconds
per_controller_bandwidth = 76.8             # In GB/s
[perf_model/cxl/memory_expander_0/dram/queue_model]
enabled = true
type = history_list

[queue_model/history_list]
max_list_size = 1024
analytical_model_enabled = false



[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus] # for multi-core communication
# as shown by chips and cheese blog as dram bottleneck for DDR5-6000
# https://chipsandcheese.com/2022/11/08/amds-zen-4-part-2-memory-subsystem-and-conclusion/
bandwidth = 145.7 # in GB/s. 72.85 GB/s per direction per connected chip pair => 145.7 GB/s
ignore_local_traffic = false # Memory controllers are off-chip, so traffic from core0 to dram0 still use the QPI links

